{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares: Finite Sample Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b8d9c",
   "metadata": {},
   "source": [
    "\n",
    "We continue with properties of OLS. We derive its finite-sample exact distribution which can\n",
    "be used for statistical inference. The Gauss-Markov theorem justifies\n",
    "the optimality of OLS under the classical assumptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9119de",
   "metadata": {},
   "source": [
    "[Exercise. don't need to translate yet] Two groups, control and treatment. If there are people in the control group and $n-1$ people in the treatment group. What is the consequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57b5e0",
   "metadata": {},
   "source": [
    "基于我们已经在前一讲中学习过的线性投影模型\n",
    "\n",
    "$$\n",
    "\\begin{aligned}y & =x'\\beta+e\\end{aligned}, \n",
    "$$\n",
    "\n",
    "投影系数 $\\beta$ 可以写作\n",
    "\n",
    "$$\n",
    "\\beta=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right]. \n",
    "$$(eqn:1)\n",
    "\n",
    "我们从 $\\left(y,x\\right)$ 的联合分布中取出一个观测值, 记作 $\\left(y_{i},x_{i}\\right)$. 重复此过程 $n$ 次得到 $n$ 个观测值, 即 $i=1,\\ldots,n$, 那么我们随即就得到了一个样本 $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbed40",
   "metadata": {},
   "source": [
    "样本均值 (sample mean) 是总体均值 (population mean) 的天然估计量. 将式 {eq}`eqn:1` 中总体均值 $E\\left[\\cdot\\right]$ 替换为样本均值 $\\frac{1}{n}\\sum_{i=1}^{n}\\cdot$, 那么相应的, 最小二乘法系数的估计值就可以写作\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\beta} & =\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}\\\\\n",
    " & =\\left(\\frac{X'X}{n}\\right)^{-1}\\frac{X'y}{n}=\\left(X'X\\right)^{-1}X'y.\\end{aligned}\n",
    "$$\n",
    "\n",
    "上式假设 $X'X$ 是可逆的. 这是一种理解最小二乘法的观点. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b49522",
   "metadata": {},
   "source": [
    "```{prf:remark}\n",
    ":label: remark31\n",
    "样本 $\\left(y_{i},x_{i}\\right)$ 到底是随机的呢？还是固定的呢？\n",
    "\n",
    "——在我们观测之前, 样本是随机变量, 而随机变量的值是不确定的. 当我们谈起样本的统计学性质时, 我们必须将其视之为随机变量, 因为只有随机变量才有统计学性质, 固定值的统计学性质是无意义的. 而在我们观测之后, 样本的值就确定下来了, 成为固定值, 不能再更改. \n",
    "\n",
    "```\n",
    "\n",
    "```{prf:remark}\n",
    ":label: remark32\n",
    "在实际操作中, 我们手中只有一些给定的数据 (当然, 现在的大数据也可以将文本, 照片声音和图像处理成为数据, 这些数据在计算机当中用0和1来表示) . 我们把这些数据扔给计算机, 让计算机给出一个结果. 在统计学意义上, 我们认为这些数字是从一个概率分布上得出的**思想实验**结果. 思想实验是一个学术用语, 说白了, 它就是一个故事. 在公理体系统治的概率论当中, 这个故事在数学上是自洽的. 但是要知道，数学本身是一个套套逻辑 (tautology) , 而不是科学. \n",
    "\n",
    "而概率模型的科学价值恰恰在于, 它到底在多大程度上能够逼近事实的真相？以及, 它是不是能够帮我们预测一些真相？在这门课中, 我们假设数据来自于某种机制, 我们把这种机制当成真相. 比如, 在线性回归当中, $\\left(y,x\\right)$ 的联合分布就是我们头脑中的真相. 而我们想要研究的线性投影系数 $\\beta$ 即为此真相的某个侧面表现形式. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d2613",
   "metadata": {},
   "source": [
    "样本均值 (sample mean) 是总体均值 (population mean) 的天然估计量. 将式 {eq}`eqn:1` 中总体均值 $E\\left[\\cdot\\right]$ 替换为样本均值 $\\frac{1}{n}\\sum_{i=1}^{n}\\cdot$, 那么相应的, 最小二乘法系数的估计值就可以写作\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\beta} & =\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}\\\\\n",
    " & =\\left(\\frac{X'X}{n}\\right)^{-1}\\frac{X'y}{n}=\\left(X'X\\right)^{-1}X'y.\\end{aligned}\n",
    "$$\n",
    "\n",
    "上式假设 $X'X$ 是可逆的. 这是一种理解最小二乘法的观点. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86dd1a",
   "metadata": {},
   "source": [
    "\n",
    "## Finite Sample Distribution\n",
    "\n",
    "We can show the finite-sample exact distribution of $\\widehat{\\beta}$\n",
    "assuming the error term follows a Gaussian distribution. *Finite sample\n",
    "distribution* means that the distribution holds for any $n$; it is in\n",
    "contrast to *asymptotic distribution*, which is a large sample\n",
    "approximation to the finite sample distribution. We first review some\n",
    "properties of a generic jointly normal random vector.\n",
    "\n",
    "<span id=\"fact31\" label=\"fact31\">\\[fact31\\]</span> Let\n",
    "$z\\sim N\\left(\\mu,\\Omega\\right)$ be an $l\\times1$ random vector with a\n",
    "positive definite variance-covariance matrix $\\Omega$. Let $A$ be an\n",
    "$m\\times l$ non-random matrix where $m\\leq l$. Then\n",
    "$Az\\sim N\\left(A\\mu,A\\Omega A'\\right)$.\n",
    "\n",
    "<span id=\"fact32\" label=\"fact32\">\\[fact32\\]</span>If\n",
    "$z\\sim N\\left(0,1\\right)$, $w\\sim\\chi^{2}\\left(d\\right)$ and $z$ and $w$\n",
    "are independent. Then $\\frac{z}{\\sqrt{w/d}}\\sim t\\left(d\\right)$.\n",
    "\n",
    "The OLS estimator\n",
    "$$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'Y=\\left(X'X\\right)^{-1}X'\\left(X'\\beta+e\\right)=\\beta+\\left(X'X\\right)^{-1}X'e,$$\n",
    "and its conditional distribution can be written as $$\\begin{aligned}\n",
    "\\widehat{\\beta}|X & =\\beta+\\left(X'X\\right)^{-1}X'e|X\\\\\n",
    " & \\sim\\beta+\\left(X'X\\right)^{-1}X'\\cdot N\\left(0_{n},\\gamma I_{n}\\right)\\\\\n",
    " & \\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}X'X\\left(X'X\\right)^{-1}\\right)\\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}\\right)\\end{aligned}$$\n",
    "by Fact\n",
    "<a href=\"#fact31\" data-reference-type=\"ref\" data-reference=\"fact31\">[fact31]</a>.\n",
    "The $k$-th element of the vector coefficient\n",
    "$$\\widehat{\\beta}_{k}|X=\\eta_{k}'\\widehat{\\beta}|X\\sim N\\left(\\beta_{k},\\gamma\\eta_{k}'\\left(X'X\\right)^{-1}\\eta_{k}\\right)\\sim N\\left(\\beta_{k},\\gamma\\left[\\left(X'X\\right)^{-1}\\right]_{kk}\\right),$$\n",
    "where $\\eta_{k}=\\left(1\\left\\{ l=k\\right\\} \\right)_{l=1,\\ldots,K}$ is\n",
    "the selector of the $k$-th element.\n",
    "\n",
    "In reality, $\\sigma^{2}$ is an unknown parameter, and\n",
    "$$s^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)=e'M_{X}e/\\left(n-K\\right)$$\n",
    "is an unbiased estimator of $\\gamma$. (Because \n",
    "\n",
    "$$\\begin{aligned}\n",
    "E\\left[s^{2}|X\\right] & =\\frac{1}{n-K}E\\left[e'M_{X}e|X\\right]=\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[e'M_{X}e|X\\right]\\right)\\\\\n",
    " & =\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[M_{X}ee'|X\\right]\\right)=\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}E\\left[ee'|X\\right]\\right)\\\\\n",
    " & =\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}\\gamma I_{n}\\right)=\\frac{\\gamma}{n-K}\\mathrm{trace}\\left(M_{X}\\right)=\\gamma\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we use the property of trace\n",
    "$\\mathrm{trace}\\left(AB\\right)=\\mathrm{trace}\\left(BA\\right)$.)\n",
    "\n",
    "Under the null hypothesis $H_{0}:\\beta_{k}=\\beta_{k}^{*}$, where\n",
    "$\\beta_{k}^{*}$ is the hypothesized value we want to test. We can\n",
    "construct a $t$-statistic\n",
    "$$T_{k}=\\frac{\\widehat{\\beta}_{k}-\\beta_{k}^{*}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}},$$\n",
    "which is *infeasible* is that sense that it can be directly computed\n",
    "from the data because there is no unknown object in this statistic. When\n",
    "the hypothesis is true, $\\beta_{k}=\\beta_{k}^{*}$ and thus\n",
    "\n",
    "\\label{eq:t-stat}\n",
    "$$\\begin{aligned}\n",
    "T_{k} & =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}} \\\\\n",
    " & =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}\\cdot\\frac{\\sqrt{\\sigma^{2}}}{\\sqrt{s^{2}}} \\\\\n",
    " & =\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{0,k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we introduce the population quantity $\\sigma^{2}$ into the second\n",
    "equality to help derive the distribution of the numerator and the\n",
    "denominator of the last expression. The numerator\n",
    "$$\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}\\sim N\\left(0,1\\right),$$\n",
    "and the denominator\n",
    "$\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}$ follows\n",
    "$\\sqrt{\\frac{1}{n-K}\\chi^{2}\\left(n-K\\right)}$. Moreover, because\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\\widehat{\\beta}-\\beta\\\\\n",
    "\\widehat{e}\n",
    "\\end{bmatrix} & =\\begin{bmatrix}\\left(X'X\\right)^{-1}X'e\\\\\n",
    "M_{X}e\n",
    "\\end{bmatrix}=\\begin{bmatrix}\\left(X'X\\right)^{-1}X'\\\\\n",
    "M_{X}\n",
    "\\end{bmatrix}e\\\\\n",
    " & \\sim\\begin{bmatrix}\\left(X'X\\right)^{-1}X'\\\\\n",
    "M_{X}\n",
    "\\end{bmatrix}\\cdot N\\left(0,\\gamma I_{n}\\right)\\sim N\\left(0,\\gamma\\begin{bmatrix}\\left(X'X\\right)^{-1} & 0\\\\\n",
    "0 & M_{X}\n",
    "\\end{bmatrix}\\right)\\end{aligned}\n",
    "$$ \n",
    "\n",
    "are jointly normal with zero\n",
    "off-diagonal blocks, $\\left(\\widehat{\\beta}-\\beta\\right)$ and\n",
    "$\\widehat{e}$ are statistically independent. (This claim is true,\n",
    "although the covariance matrix of the $\\widehat{e}$ is singular.) Given\n",
    "that $X$ is viewed as if non-random, the numerator and the denominator\n",
    "of\n",
    "(<a href=\"#eq:t-stat\" data-reference-type=\"ref\" data-reference=\"eq:t-stat\">[eq:t-stat]</a>)\n",
    "are statistically independent as well is a function since the former is\n",
    "a function of $\\left(\\widehat{\\beta}-\\beta\\right)$ and latter is a\n",
    "function of $\\widehat{e}$. (Alternatively, the statistically independent\n",
    "can be verified by Basu’s theorem, See Appendix\n",
    "<a href=\"#subsec:Basu&#39;s-Theorem\" data-reference-type=\"ref\" data-reference=\"subsec:Basu&#39;s-Theorem\">[subsec:Basu's-Theorem]</a>.)\n",
    "As a result, we conclude $T_{k}\\sim t\\left(n-K\\right)$ by Fact\n",
    "<a href=\"#fact32\" data-reference-type=\"ref\" data-reference=\"fact32\">[fact32]</a>.\n",
    "This finite sample distribution allows us to conduct statistical\n",
    "inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0f2a9",
   "metadata": {},
   "source": [
    "\n",
    "## Mean and Variance<span id=\"mean-and-variance\" label=\"mean-and-variance\">\\[mean-and-variance\\]</span>\n",
    "\n",
    "Now we relax the normality assumption and statistical independence.\n",
    "Instead, we represent the regression model as $Y=X\\beta+e$ and\n",
    "$$\\begin{aligned}\n",
    "E[e|X] & =0_{n}\\\\\n",
    "\\mathrm{var}\\left[e|X\\right] & =E\\left[ee'|X\\right]=\\sigma^{2}I_{n}.\\end{aligned}$$\n",
    "where the first condition is the *mean independence* assumption, and the\n",
    "second condition is the *homoskedasticity* assumption. These assumptions\n",
    "are about the first and second *moments* of $e_{i}$ conditional on\n",
    "$x_{i}$. Unlike the normality assumption, they do not restrict the\n",
    "distribution of $e_{i}$.\n",
    "\n",
    "-   Unbiasedness: $$\\begin{aligned}\n",
    "    E\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(X'X\\right)^{-1}XY|X\\right]=E\\left[\\left(X'X\\right)^{-1}X\\left(X'\\beta+e\\right)|X\\right]\\\\\n",
    "     & =\\beta+\\left(X'X\\right)^{-1}XE\\left[e|X\\right]=\\beta.\\end{aligned}$$\n",
    "    By the law of iterated expectations, the unconditional expectation\n",
    "    $E\\left[\\widehat{\\beta}\\right]=E\\left[E\\left[\\widehat{\\beta}|X\\right]\\right]=\\beta.$\n",
    "    Unbiasedness does not rely on homoskedasticity.\n",
    "\n",
    "-   Variance:\n",
    "-   \n",
    "    $$\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)'|X\\right]\\\\\n",
    "     & =E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)'|X\\right]\\\\\n",
    "     & =E\\left[\\left(X'X\\right)^{-1}X'ee'X\\left(X'X\\right)^{-1}|X\\right]\\\\\n",
    "     & =\\left(X'X\\right)^{-1}X'E\\left[ee'|X\\right]X\\left(X'X\\right)^{-1}\n",
    "    \\end{aligned}$$ \n",
    "    \n",
    "    where the second equality holds as\n",
    "\n",
    "-   Under the assumption of homoskedasticity, it can be simplified as\n",
    "    $$\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =\\left(X'X\\right)^{-1}X'\\left(\\sigma^{2}I_{n}\\right)X\\left(X'X\\right)^{-1}\\\\\n",
    "     & =\\sigma^{2}\\left(X'X\\right)^{-1}X'I_{n}X\\left(X'X\\right)^{-1}\\\\\n",
    "     & =\\sigma^{2}\\left(X'X\\right)^{-1}.\n",
    "    \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a14af6",
   "metadata": {},
   "source": [
    "\n",
    "(Heteroskedasticity) If $e_{i}=x_{i}u_{i}$, where $x_{i}$ is a scalar\n",
    "random variable, $u_{i}$ is statistically independent of $x_{i}$,\n",
    "$E\\left[u_{i}\\right]=0$ and $E\\left[u_{i}^{2}\\right]=\\sigma_{u}^{2}$.\n",
    "Then\n",
    "$E\\left[e_{i}|x_{i}\\right]=E\\left[x_{i}u_{i}|x_{i}\\right]=x_{i}E\\left[u_{i}|x_{i}\\right]=0$\n",
    "but\n",
    "$E\\left[e_{i}^{2}|x_{i}\\right]=E\\left[x_{i}^{2}u_{i}^{2}|x_{i}\\right]=x_{i}^{2}E\\left[u_{i}^{2}|x_{i}\\right]=\\sigma_{u}^{2}x_{i}^{2}$\n",
    "is a function of $x_{i}$. We say $e_{i}^{2}$ is a heteroskedastic error.\n",
    "\n",
    "\\*\\*knitr\\*\\*\n",
    "\n",
    "It is important to notice that independently and identically distributed\n",
    "sample (iid) $\\left(y_{i},x_{i}\\right)$ does not imply homoskedasticity.\n",
    "Homoskedasticity or heteroskedasticity is about the relationship between\n",
    "$\\left(x_{i},e_{i}=y_{i}-\\beta x\\right)$ within an observation, whereas\n",
    "iid is about the relationship between $\\left(y_{i},x_{i}\\right)$ and\n",
    "$\\left(y_{j},x_{j}\\right)$ for $i\\neq j$ across observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a8324",
   "metadata": {},
   "source": [
    "\n",
    "## Gauss-Markov Theorem\n",
    "\n",
    "Gauss-Markov theorem is concerned about the optimality of OLS. It\n",
    "justifies OLS as the efficient estimator among all linear unbiased ones.\n",
    "*Efficient* here means that it enjoys the smallest variance in a family\n",
    "of estimators.\n",
    "\n",
    "We have shown that OLS is unbiased in that\n",
    "$E\\left[\\widehat{\\beta}\\right]=\\beta$. There are numerous linearly\n",
    "unbiased estimators. For example, $\\left(Z'X\\right)^{-1}Z'y$ for\n",
    "$z_{i}=x_{i}^{2}$ is unbiased because\n",
    "$E\\left[\\left(Z'X\\right)^{-1}Z'y\\right]=E\\left[\\left(Z'X\\right)^{-1}Z'\\left(X\\beta+e\\right)\\right]=\\beta$.\n",
    "We cannot say OLS is better than those other unbiased estimators because\n",
    "they are all unbiased — they are equally good at this aspect. We move to\n",
    "the second order property of variance: an estimator is better if its\n",
    "variance is smaller.\n",
    "\n",
    "For two generic random vectors $X$ and $Y$ of the same size, we say\n",
    "$X$’s variance is smaller or equal to $Y$’s variance if\n",
    "$\\left(\\Omega_{Y}-\\Omega_{X}\\right)$ is a positive semi-definite matrix.\n",
    "The comparison is defined this way because for any non-zero constant\n",
    "vector $c$, the variance of the linear combination of $X$\n",
    "$$\\mathrm{var}\\left(c'X\\right)=c'\\Omega_{X}c\\leq c'\\Omega_{Y}c=\\mathrm{var}\\left(c'Y\\right)$$\n",
    "is no bigger than the same linear combination of $Y$.\n",
    "\n",
    "Let $\\tilde{\\beta}=A'y$ be a generic linear estimator, where $A$ is any\n",
    "$n\\times K$ functions of $X$. As\n",
    "$$E\\left[A'y|X\\right]=E\\left[A'\\left(X\\beta+e\\right)|X\\right]=A'X\\beta.$$\n",
    "So the linearity and unbiasedness of $\\tilde{\\beta}$ implies\n",
    "$A'X=I_{n}$. Moreover, the variance\n",
    "\n",
    "$$\n",
    "var\\left(A'y|X\\right)=E\\left[\\left(A'y-\\beta\\right)\\left(A'y-\\beta\\right)'|X\\right]=E\\left[A'ee'A|X\\right]=\\sigma^{2}A'A.\n",
    "$$\n",
    "\n",
    "Let $C=A-X\\left(X'X\\right)^{-1}.$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}A'A-\\left(X'X\\right)^{-1} & =\\left(C+X\\left(X'X\\right)^{-1}\\right)'\\left(C+X\\left(X'X\\right)^{-1}\\right)-\\left(X'X\\right)^{-1}\\\\\n",
    " & =C'C+\\left(X'X\\right)^{-1}X'C+C'X\\left(X'X\\right)^{-1}\\\\\n",
    " & =C'C,\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "where the last equality follows as\n",
    "$$\\left(X'X\\right)^{-1}X'C=\\left(X'X\\right)^{-1}X'\\left(A-X\\left(X'X\\right)^{-1}\\right)=\\left(X'X\\right)^{-1}-\\left(X'X\\right)^{-1}=0.$$\n",
    "Therefore $A'A-\\left(X'X\\right)^{-1}$ is a positive semi-definite\n",
    "matrix. The variance of any $\\tilde{\\beta}$ is no smaller than the OLS\n",
    "estimator $\\widehat{\\beta}$. The above derivation shows OLS achieves the\n",
    "smallest variance among all linear unbiased estimators.\n",
    "\n",
    "Homoskedasticity is a restrictive assumption. Under homoskedasticity,\n",
    "$\\mathrm{var}\\left[\\widehat{\\beta}\\right]=\\sigma^{2}\\left(X'X\\right)^{-1}$.\n",
    "Popular estimator of $\\sigma^{2}$ is the sample mean of the residuals\n",
    "$\\widehat{\\sigma}^{2}=\\frac{1}{n}\\widehat{e}'\\widehat{e}$ or the\n",
    "unbiased one $s^{2}=\\frac{1}{n-K}\\widehat{e}'\\widehat{e}$. Under\n",
    "heteroskedasticity, Gauss-Markov theorem does not apply.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa5d52",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "The exact distribution under the normality assumption of the error term\n",
    "is the classical statistical results. The Gauss Markov theorem holds\n",
    "under two crucial assumptions: linear CEF and homoskedasticity.\n",
    "\n",
    "**Historical notes**: MLE was promulgated and popularized by Ronald\n",
    "Fisher (1890–1962). He was a major contributor of the frequentist\n",
    "approach which dominates mathematical statistics today, and he sharply\n",
    "criticized the Bayesian approach. Fisher collected the iris flower\n",
    "dataset of 150 observations in his biological study in 1936, which can\n",
    "be displayed in R by typing `iris`. Fisher invented the many concepts in\n",
    "classical mathematical statistics, such as sufficient statistic,\n",
    "ancillary statistic, completeness, and exponential family, etc.\n",
    "\n",
    "**Further reading**: @phillips1983exact offered a comprehensive\n",
    "treatment of exact small sample theory in econometrics. After that,\n",
    "theoretical studies in econometrics swiftly shifted to large sample\n",
    "theory, which we will introduce in the next chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28f4c2",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix\n",
    "\n",
    "### Joint Normal Distribution\n",
    "\n",
    "It is arguable that normal distribution is the most frequently\n",
    "encountered distribution in statistical inference, as it is the\n",
    "asymptotic distribution of many popular estimators. Moreover, it boasts\n",
    "some unique features that facilitates the calculation of objects of\n",
    "interest. This note summaries a few of them.\n",
    "\n",
    "An $n\\times1$ random vector $Y$ follows a joint normal distribution\n",
    "$N\\left(\\mu,\\Sigma\\right)$, where $\\mu$ is an $n\\times1$ vector and\n",
    "$\\Sigma$ is an $n\\times n$ symmetric positive definite matrix. The\n",
    "probability density function is\n",
    "$$f_{y}\\left(y\\right)=\\left(2\\pi\\right)^{-n/2}\\left(\\mathrm{det}\\left(\\Sigma\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2}\\left(y-\\mu\\right)'\\Sigma^{-1}\\left(y-\\mu\\right)\\right)$$\n",
    "where $\\mathrm{det}\\left(\\cdot\\right)$ is the determinant of a matrix.\n",
    "The moment generating function is\n",
    "$$M_{y}\\left(t\\right)=\\exp\\left(t'\\mu+\\frac{1}{2}t'\\Sigma t\\right).$$\n",
    "\n",
    "We will discuss the relationship between two components of a random\n",
    "vector. To fix notation, $$Y=\\left(\\begin{array}{c}\n",
    "Y_{1}\\\\\n",
    "Y_{2}\n",
    "\\end{array}\\right)\\sim N\\left(\\left(\\begin{array}{c}\n",
    "\\mu_{1}\\\\\n",
    "\\mu_{2}\n",
    "\\end{array}\\right),\\left(\\begin{array}{cc}\n",
    "\\Sigma_{11} & \\Sigma_{12}\\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{array}\\right)\\right)$$ where $Y_{1}$ is an $m\\times1$ vector, and\n",
    "$Y_{2}$ is an $\\left(n-m\\right)\\times1$ vector. $\\mu_{1}$ and $\\mu_{2}$\n",
    "are the corresponding mean vectors, and $\\Sigma_{ij}$, $j=1,2$ are the\n",
    "corresponding variance and covariance matrices. From now on, we always\n",
    "maintain the assumption that $Y=\\left(Y_{1}',Y_{2}'\\right)'$ is jointly\n",
    "normal.\n",
    "\n",
    "Fact\n",
    "<a href=\"#fact31\" data-reference-type=\"ref\" data-reference=\"fact31\">[fact31]</a>\n",
    "immediately implies a convenient feature of the normal distribution.\n",
    "Generally speaking, if we are given a joint pdf of two random variables\n",
    "and intend to find the marginal distribution of one random variables, we\n",
    "need to integrate out the other variable from the joint pdf. However, if\n",
    "the variables are jointly normal, the information of the other random\n",
    "variable is irrelevant to the marginal distribution of the random\n",
    "variable of interest. We only need to know the partial information of\n",
    "the part of interest, say the mean $\\mu_{1}$ and the variance\n",
    "$\\Sigma_{11}$ to decide the marginal distribution of $Y_{1}$.\n",
    "\n",
    "<span id=\"fact:marginal\"\n",
    "label=\"fact:marginal\">\\[fact:marginal\\]</span>The marginal distribution\n",
    "$Y_{1}\\sim N\\left(\\mu_{1},\\Sigma_{11}\\right)$.\n",
    "\n",
    "This result is very convenient if we are interested in some component if\n",
    "an estimator, but not the entire vector of the estimator. For example,\n",
    "the OLS estimator of the linear regression model\n",
    "$y_{i}=x_{i}'\\beta+e_{i}$, under the classical assumption of (i) random\n",
    "sample; (ii) independence of $z_{i}$ and $e_{i}$; (iii)\n",
    "$e_{i}\\sim N\\left(0,\\gamma\\right)$ is\n",
    "$$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y,$$ and the finite sample\n",
    "exact distribution of $\\widehat{\\beta}$ is\n",
    "$$\\left(\\widehat{\\beta}-\\beta\\right)|X\\sim N\\left(0,\\gamma\\left(X'X\\right)^{-1}\\right)$$\n",
    "If we are interested in the inference of only the $j$-th component of\n",
    "$\\beta_{0}^{\\left(j\\right)}$, then from Fact\n",
    "<a href=\"#fact:marginal\" data-reference-type=\"ref\" data-reference=\"fact:marginal\">[fact:marginal]</a>,\n",
    "$$\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\left(X'X\\right)_{kk}^{-1}\\sim N\\left(0,\\gamma\\right)$$\n",
    "where $\\left[\\left(X'X\\right)^{-1}\\right]_{kk}$ is the $k$-th diagonal\n",
    "element of $\\left(X'X\\right)^{-1}$. The marginal distribution is\n",
    "independent of the other components. This saves us from integrating out\n",
    "the other components, which could be troublesome if the dimension of the\n",
    "vector is high.\n",
    "\n",
    "Generally, zero covariance of two random variables only indicates that\n",
    "they are uncorrelated, whereas full statistical independence is a much\n",
    "stronger requirement. However, if $Y_{1}$ and $Y_{2}$ are jointly\n",
    "normal, then zero covariance is equivalent to full independence.\n",
    "\n",
    "If $\\Sigma_{12}=0$, then $Y_{1}$ and $Y_{2}$ are independent.\n",
    "\n",
    "If $\\Sigma$ is invertible, then\n",
    "$Y'\\Sigma^{-1}Y\\sim\\chi^{2}\\left(\\mathrm{rank}\\left(\\Sigma\\right)\\right)$.\n",
    "\n",
    "The last result, which is useful in linear regression, is that if\n",
    "$Y_{1}$ and $Y_{2}$ are jointly normal, the conditional distribution of\n",
    "$Y_{1}$ on $Y_{2}$ is still jointly normal, with the mean and variance\n",
    "specified as in the following fact.\n",
    "\n",
    "$Y_{1}|Y_{2}\\sim N\\left(\\mu_{1}+\\Sigma_{12}\\Sigma_{22}^{-1}\\left(Y_{2}-\\mu_{2}\\right),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right)$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
